---
title: "The shame of using inhuman assistance"
date: 2026-01-19
draft: false
summary: "Is using LLM technology degenerative, a fad or just the next step in technology evolution?"
tags: ["technology", "large-language-models", "ai"]
categories: ["Thoughts"]
weight: 1
---

It is time to address the proverbial elephant in the room: this blog is supported by Large Language Models. I leverage these tools for essential tasks such as spell-checking, synonym discovery, and refining syntax, as well as for generating supporting imagery.

I believe in full transparency regarding my process; I want to ensure there is no ambiguity about how these tools augment my work. In this post, we will explore my creative process and how I integrate digital assistance into my writing, my professional life, and my daily routines.

There was a time when information felt like a hidden gem; finding it was not necessarily rare, but sifting through the noise to determine what was accurate was a laborious task. For those of us from older generations of the internet, we were taught to view any online information as suspect at best. In my experience, there were better sources and worse sources, but nothing was ever accepted as absolute truth.

Developing the ability to discern trustworthiness was paramount. Today, there is a fear that the arrival of Large Language Models might erode our capacity to form independent opinions. However, for those who came of age believing that 'nothing online can be trusted,' the playing field has simply shifted; the sport and the judges remain exactly the same.

On a daily basis, the primary function of the Large Language Model is to be the fire that projects the shadow on the cave wall. It provides a plausible, yet mediated representation. A narrative that still requires the observer to understand the limitations of their own perception. Even with these tools, we should always operate under the original digital truth: nothing online can be trusted.

This perspective provides the most pragmatic angle for navigating a daily life supported by what many consider a destructive technology. To put it bluntly, and regardless of which side of the fence you sit on, using this technology today feels a bit like eating meat. One can acknowledge that it supports a cold, often inhumane industry, yet still find that it tastes good. The utility is simply undeniable, even if the origin is contested.

The way I have incorporated Large Language Models into my life is quite similar to how I used search engines before this technology emerged. The workflow remains the same: search, identify sources, and establish a baseline for the data's trustworthiness. It is important to note that, on a daily basis, my use of the technology is fairly rudimentary. In this context, it is little more than a glorified chat with a search engine.

In sharp contrast to those mundane tasks is the dialogue-esque banter I use to craft texts for both my personal blog and my correspondence. In this space, the technology performs much closer to its promised intelligence, displaying a convincing sense of rhyme and reason. This very post is being created step by step: I first write a paragraph, then discuss and refine it with the help of what can only be described as a highly advanced and informative thesaurus. Since my understanding of the intricacies (I actually had to ask the AI for the spelling of that word just now) of both Swedish and English grammar is bad at best, the technology allows me to write how the text feels in my mind.

It is less about the machine writing for me and more about it acting as a mirror for my intent. I like to believe that if I had stayed at University to study further, I might have been more of a wordsmith than I am today. For me, this makes the technology assistive rather than degenerative. It provides the scaffolding I need to properly translate my thoughts onto the digital page. 

This approach stands in stark contrast to those who use the technology to flood the internet with meta-content, polluting our digital spaces with what can only be described as synthetic sludge. I fear a future where pure generation, devoid of human participation, dilutes the global corpus of information. This shift threatens to leave the owners of massive data silos in a position to dictate the availability and shape of human knowledge. Regardless, I want to be clear on one point: my usage of the technology will always be assistive rather than generative when it comes to words.

> You are about as artistic as a dump

That assessment is exactly why I have embraced generative side of the models for visuals of this blog. While my hands might lack the dexterity to paint or sketch, my mind does not lack the vision. By using image generation, I can finally bypass the "dump-like" limitations of my physical artistry. I use these tools to create the headers and illustrations that accompany my words, ensuring that the visual experience matches the tone of the text. In this context, the model is not replacing an artist: it is finally giving one a set of tools that actually work.

Yet, delegating a task entirely to a tool, rather than participating in the rough sketches myself, feels like a personal failing. While I am comfortable using these models for language, I draw a firm line at the generation of actual code. In my view, the technology adds value when used as a research tool, a text editor, or a debugger. However, when used unthinkingly or without a specific aim, it becomes a liability. I am acutely aware of this in my professional life, where I work to ensure that the bastions of our code are safe, secure, and free of malice.

There is a lingering feeling that we are at the beginning of an "age of slop," a time where code is produced at a higher pace and within exponentially larger code-bases. To be perfectly explicit: using a Large Language Model to fully generate a solution without understanding the underlying concepts is dangerous. The concept of "vibe coding" is essentially the same as saying:

> I just searched for and used every example I found on Stack Overflow.

While there is no doubt that such code might work, even as intended, the risks are high. Without a human understanding the "why" behind the logic, the resulting work is frequently insecure, ineffective, and incredibly inefficient. When I started my professional career in Information Technology, I followed individuals who had to learn their craft by reading books. This foundation did not prevent my mentors or me from using technology in increasingly efficient and advanced ways. Believing that Large Language Models are an absolute negative is just as incorrect as thinking they are a pure positive.
